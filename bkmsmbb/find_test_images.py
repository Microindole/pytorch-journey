import os
import requests
import time
import argparse
from duckduckgo_search import DDGS

# --- è®¾ç½® ---
# é»˜è®¤ä¿å­˜å›¾ç‰‡çš„ç›®å½•
DEFAULT_SAVE_DIR = "./test_images_found"
# è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}


def search_and_download(query, limit, save_dir):
    """
    æ ¹æ®ç»™å®šçš„æŸ¥è¯¢è¯ï¼Œæœç´¢å¹¶ä¸‹è½½å›¾ç‰‡ã€‚

    :param query: æœç´¢å…³é”®è¯ (ä¾‹å¦‚ "Pikachu", "Greymon")
    :param limit: è¦ä¸‹è½½çš„å›¾ç‰‡æ•°é‡
    :param save_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
    """
    print(f"\nğŸ” æ­£åœ¨ä¸ºå…³é”®è¯ '{query}' æœç´¢ {limit} å¼ å›¾ç‰‡...")

    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(save_dir, exist_ok=True)

    download_count = 0
    try:
        # ä½¿ç”¨ DDGS().images() è¿›è¡Œå›¾ç‰‡æœç´¢
        with DDGS() as ddgs:
            search_results = [r for r in ddgs.images(query, max_results=limit * 2)]  # å¤šè·å–ä¸€äº›ç»“æœä»¥å¤‡ä¸‹è½½å¤±è´¥

        if not search_results:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡ç»“æœã€‚")
            return

        print(f"âœ… æ‰¾åˆ°äº† {len(search_results)} ä¸ªç»“æœï¼Œå¼€å§‹ä¸‹è½½å‰ {limit} ä¸ª...")

        for i, result in enumerate(search_results):
            if download_count >= limit:
                break

            image_url = result.get('image')
            if not image_url:
                continue

            try:
                # ç”Ÿæˆæ–‡ä»¶å
                # è·å–æ–‡ä»¶åç¼€åï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º .jpg
                file_ext = os.path.splitext(os.path.basename(image_url))[1]
                if not file_ext:
                    file_ext = '.jpg'

                # æ¸…ç†æŸ¥è¯¢è¯ä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†
                safe_query = "".join(c for c in query if c.isalnum() or c in " _-").rstrip()
                filename = f"{safe_query}_{download_count + 1}{file_ext}"
                save_path = os.path.join(save_dir, filename)

                # ä¸‹è½½å›¾ç‰‡
                response = requests.get(image_url, headers=HEADERS, stream=True, timeout=10)
                response.raise_for_status()

                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                print(f"  ({download_count + 1}/{limit}) ä¸‹è½½æˆåŠŸ: {filename}")
                download_count += 1
                time.sleep(0.2)  # çŸ­æš‚å»¶æ—¶

            except Exception as e:
                print(f"  (è·³è¿‡) ä¸‹è½½å¤±è´¥: {image_url} (åŸå› : {e})")

    except Exception as e:
        print(f"âŒ æœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    print(f"\nä¸‹è½½å®Œæˆï¼å…±ä¸‹è½½äº† {download_count} å¼ å›¾ç‰‡åˆ° '{os.path.abspath(save_dir)}' ç›®å½•ã€‚")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ä»ç½‘ä¸Šæœç´¢å¹¶ä¸‹è½½æµ‹è¯•å›¾ç‰‡ã€‚")
    parser.add_argument('-q', '--query', type=str, required=True,
                        help='è¦æœç´¢çš„å›¾ç‰‡å…³é”®è¯ï¼Œä¾‹å¦‚ "Charizard" æˆ– "Agumon"ã€‚')
    parser.add_argument('-l', '--limit', type=int, default=5, help='è¦ä¸‹è½½çš„å›¾ç‰‡æ•°é‡ (é»˜è®¤: 5)ã€‚')
    parser.add_argument('-o', '--output_dir', type=str, default=DEFAULT_SAVE_DIR,
                        help=f'å›¾ç‰‡ä¿å­˜ç›®å½• (é»˜è®¤: {DEFAULT_SAVE_DIR})ã€‚')
    args = parser.parse_args()

    search_and_download(args.query, args.limit, args.output_dir)